{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94929251-1eac-4d60-9c83-fee493f68ea2",
   "metadata": {},
   "source": [
    "# Bulding a simple chess engine with reinforcement learning\n",
    "# Outline\n",
    "- [Step 1: Importing Essential Libraries](#1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8802dbbe-7660-4d0c-81c0-81c1fc2b303e",
   "metadata": {},
   "source": [
    "<a name=\"1\"></a>\n",
    "## Step 1: Importing Essential Libraries\n",
    "\n",
    "Before building our chess engine, we need to set up the necessary tools. This step involves importing various Python libraries that will help with numerical operations, reinforcement learning, deep learning, chess logic, and data handling.\n",
    "Code Implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bf825d-30f5-451e-a888-9d33643dcb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from collections import deque, namedtuple\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import chess\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa97cab-8e11-4e0b-a12a-d71d8a7250ca",
   "metadata": {},
   "source": [
    "### Explanation of Imported Libraries:\n",
    "- `NumPy` (numpy) – Provides efficient array handling, useful for representing chess positions numerically.\n",
    "- `Gym` (gym) – Reinforcement learning environment, potentially helping with training a chess AI.\n",
    "- `Spaces` (gym.spaces) – Defines action and observation spaces, essential for AI-driven move decisions.\n",
    "- `Collections` (collections.deque, collections.namedtuple) – Used for structured data storage, such as move history.\n",
    "- `TensorFlow` (tensorflow) – Enables deep learning capabilities for position evaluation and move selection.\n",
    "- `Keras` (tensorflow.keras) – Simplifies neural network construction using layers like Dense and Input.\n",
    "- `Loss Functions` (tensorflow.keras.losses) – MSE (Mean Squared Error) helps quantify the difference between predicted and actual outcomes.\n",
    "- `Optimizers` (tensorflow.keras.optimizers) – Adam optimizer helps adjust model weights for better learning.\n",
    "- `Python-Chess` (chess) – A chess-specific library for board representation, legal moves, and game management.\n",
    "- `Random (random)` – Introduces randomness, useful for move selection or initializing weights in deep learning.\n",
    "  \n",
    "### Why This Step is Important?\n",
    "Setting up the correct libraries ensures that our chess engine has the right tools for computation, game logic, and AI-based learning. These libraries form the backbone of the project, enabling us to process moves efficiently, train models, and interact with the chessboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c22ec-d1d9-452b-a138-5943427f4380",
   "metadata": {},
   "source": [
    "## Step 2: Creating the Chess Environment\n",
    "Now that we've set up our imports, we need to design a custom chess environment using OpenAI Gym. This environment will allow reinforcement learning agents to interact with the chessboard, make moves, and receive rewards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42139e0-5b83-4211-a067-c95594c5ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ChessEnv, self).__init__()\n",
    "        self.board = chess.Board()\n",
    "        # The action space: number of possible legal moves\n",
    "        self.action_space = spaces.Discrete(4672)  # UCI move representation (a reasonable upper bound for moves)\n",
    "        \n",
    "        # Observation space: an 8x8x12 board representation (binary encoding of pieces)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8, 8, 12), dtype=np.int8)\n",
    "        self.move_count = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "    def reset(self, agent_color=chess.WHITE):\n",
    "        self.agent_color = agent_color\n",
    "        self.board.reset()\n",
    "        self.move_count = 0\n",
    "        self.prev_eval = 0.0  # Initial evaluation\n",
    "        self.reward = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        move = self._action_to_move(action)\n",
    "        self.board.push(move)\n",
    "        self.move_count += 1\n",
    "        done = self.board.is_game_over()\n",
    "        reward = self._evaluate_board(done)\n",
    "        return self._get_obs(), self.reward, done, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\" Converts the board state to an observation (8x8x12 array) \"\"\"\n",
    "        obs = np.zeros((8, 8, 12), dtype=np.int8)\n",
    "\n",
    "        # Encode each piece type on the board\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                piece = self.board.piece_at(i * 8 + j)\n",
    "                if piece:\n",
    "                    piece_type = piece.piece_type\n",
    "                    color = piece.color\n",
    "                    channel = (piece_type - 1) if color else (piece_type + 5)\n",
    "                    obs[i, j, channel] = 1\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _action_to_move(self, action):\n",
    "        \"\"\" Converts the action (an integer) into a UCI move \"\"\"\n",
    "        move = list(self.board.legal_moves)[action]\n",
    "        return move\n",
    "\n",
    "    def _evaluate_board(self, done):\n",
    "        reward = self._get_material_score()  # White's perspective\n",
    "        # Movement penalty scaled exponentially, offset by turn (White = 0, Black = 1)\n",
    "        penalty = 0.01 * (1.01 ** self.move_count)\n",
    "        reward -= penalty * ((-1) ** self.board.turn)\n",
    "        # Incremental reward\n",
    "        self.reward += reward - self.prev_eval\n",
    "        self.prev_eval = reward\n",
    "        # Add endgame reward\n",
    "        if done:\n",
    "            result = self.board.result()\n",
    "            if result == \"1-0\":\n",
    "                self.reward += 100 if self.agent_color == chess.WHITE else -100\n",
    "            elif result == \"0-1\":\n",
    "                self.reward += -100 if self.agent_color == chess.WHITE else 100\n",
    "            else:\n",
    "                self.reward = 0  # Draw\n",
    "            \n",
    "        return self.reward\n",
    "\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\" Renders the current board state (in a human-readable format) \"\"\"\n",
    "        print(self.board)\n",
    "    def _get_material_score(self):\n",
    "        \"\"\"Calculates material balance from White's perspective\"\"\"\n",
    "        piece_values = {\n",
    "            chess.PAWN: 1,\n",
    "            chess.KNIGHT: 3,\n",
    "            chess.BISHOP: 3,\n",
    "            chess.ROOK: 5,\n",
    "            chess.QUEEN: 9\n",
    "        }\n",
    "    \n",
    "        score = 0\n",
    "        for square in chess.SQUARES:\n",
    "            piece = self.board.piece_at(square)\n",
    "            if piece:\n",
    "                value = piece_values.get(piece.piece_type, 0)\n",
    "                score += value if piece.color == chess.WHITE else -value\n",
    "    \n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6871d432-38b8-40a0-bfac-ad6968b744bc",
   "metadata": {},
   "source": [
    "### Explanation of Key Components\n",
    "- `Gym Environment` (ChessEnv): Defines interaction with the chess game, including move execution and state tracking.\n",
    "- `Action Space` (spaces.Discrete(4672)): Represents possible legal moves.\n",
    "- `Observation Space` (spaces.Box): Encodes board positions in an 8x8x12 array.\n",
    "- `Reward Mechanism`: Evaluates material balance and position, incentivizing better moves.\n",
    "- `Game Reset` (reset()): Initializes a fresh game state for training.\n",
    "- `Move Execution` (step()): Allows the agent to make moves and updates the board.\n",
    "- `Rendering` (render()): Prints the board visually.\n",
    "### Why This Step is Important?\n",
    "This environment sets up a structured way to train a reinforcement learning agent by defining game mechanics, move selection, and reward evaluation. It's crucial for allowing AI to learn strategies through gameplay.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5385a99d-2185-43f0-80d6-f0e9832c7f67",
   "metadata": {},
   "source": [
    "## Step 3: Testing the Chess Environment and Reward System\n",
    "Before training our AI, we need to verify that:\n",
    "- The environment correctly applies legal moves.\n",
    "- Rewards are calculated properly based on board evaluation.\n",
    "- The game progresses logically between White and Black.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2708c-2835-4c02-bf7a-e8ced254b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ChessEnv()\n",
    "n = 0\n",
    "done = False\n",
    "reward = 0\n",
    "x.reset(chess.WHITE)\n",
    "i=0\n",
    "while not done:\n",
    "    m = reward\n",
    "    if x.board.turn:\n",
    "        print(\"white's turn\")\n",
    "    else:\n",
    "        print(\"black's turn\")\n",
    "    legal_moves = list(x.board.legal_moves)\n",
    "    action = random.randint(0, len(legal_moves) - 1)\n",
    "    obs, reward, done, _ = x.step(action)\n",
    "    print(\"Reward: \", round(reward,6))\n",
    "    print(\"difference: \",round(reward - m,6) )\n",
    "    #print(x.board)\n",
    "    print(\"===\")\n",
    "    i+=1\n",
    "print (\"number of turns:\", i/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def02e57-c841-4d12-ad78-449eb6282bca",
   "metadata": {},
   "source": [
    "### What This Code Does\n",
    "- Initializes the chess environment (ChessEnv()).\n",
    "- Runs a loop until the game ends (done == False).\n",
    "- Selects and applies a random legal move on each turn.\n",
    "- Prints the turn information (White or Black).\n",
    "- Displays the calculated reward to verify that reward logic works.\n",
    "### Why This Step is Important?\n",
    "This test ensures that:\n",
    "- The environment correctly recognizes legal moves.\n",
    "- The agent receives meaningful rewards based on board evaluation.\n",
    "- The game transitions properly between turns.\n",
    "- The reward system provides useful feedback before integrating AI training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14a636-4cf0-4c47-a79d-64e844463536",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a89802-0057-42d9-af07-46979d41c3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
