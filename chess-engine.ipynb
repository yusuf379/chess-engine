{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442a9d3a-21cc-4937-84cd-efa0f5b43521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from collections import deque, namedtuple\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import chess\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42139e0-5b83-4211-a067-c95594c5ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChessEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(ChessEnv, self).__init__()\n",
    "        self.board = chess.Board()\n",
    "        # The action space: number of possible legal moves\n",
    "        self.action_space = spaces.Discrete(4672)  # UCI move representation (a reasonable upper bound for moves)\n",
    "        \n",
    "        # Observation space: an 8x8x12 board representation (binary encoding of pieces)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8, 8, 12), dtype=np.int8)\n",
    "        self.move_count = 0\n",
    "        \n",
    "    def reset(self, agent_color=chess.WHITE):\n",
    "        self.agent_color = agent_color\n",
    "        self.board.reset()\n",
    "        self.move_count = 0\n",
    "        self.prev_eval = self._evaluate_board()  # Initial evaluation\n",
    "        self.last_reward = 0.0\n",
    "        self.reward = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        move = self._action_to_move(action)\n",
    "        self.board.push(move)\n",
    "        self.move_count += 1\n",
    "        done = self.board.is_game_over()\n",
    "    \n",
    "        # Update the reward based on the new move\n",
    "        current_eval = self._evaluate_board()\n",
    "        \n",
    "        # Calculate the reward incrementally\n",
    "        if self.board.turn == self.agent_color or done:\n",
    "            self.reward += current_eval - self.prev_eval\n",
    "            self.prev_eval = current_eval\n",
    "            self.last_reward = self.reward  # Update last reward with incremental change\n",
    "        else:\n",
    "            # Agent just moved, keep the last reward\n",
    "            reward = self.last_reward\n",
    "        # Inside step()\n",
    "        if done:\n",
    "            result = self.board.result()\n",
    "            if result == \"1-0\":\n",
    "                self.reward += 100 if self.agent_color == chess.WHITE else -100\n",
    "            elif result == \"0-1\":\n",
    "                self.reward += -100 if self.agent_color == chess.WHITE else 100\n",
    "            else:\n",
    "                self.reward = 0  \n",
    "        \n",
    "        return self._get_obs(), round(self.reward, 4), done, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\" Converts the board state to an observation (8x8x12 array) \"\"\"\n",
    "        obs = np.zeros((8, 8, 12), dtype=np.int8)\n",
    "\n",
    "        # Encode each piece type on the board\n",
    "        for i in range(8):\n",
    "            for j in range(8):\n",
    "                piece = self.board.piece_at(i * 8 + j)\n",
    "                if piece:\n",
    "                    piece_type = piece.piece_type\n",
    "                    color = piece.color\n",
    "                    channel = (piece_type - 1) if color else (piece_type + 5)\n",
    "                    obs[i, j, channel] = 1\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def _action_to_move(self, action):\n",
    "        \"\"\" Converts the action (an integer) into a UCI move \"\"\"\n",
    "        move = list(self.board.legal_moves)[action]\n",
    "        return move\n",
    "\n",
    "    def _evaluate_board(self):\n",
    "        reward = self._get_material_score()  # White perspective\n",
    "        if self.board.is_check():\n",
    "            reward += 0.5\n",
    "        reward -= 0.1 * (self.move_count-(1-x.agent_color))\n",
    "        if self.move_count == 0: reward = 0\n",
    "    \n",
    "        # Flip evaluation if agent is black (so agent always wants to maximize reward)\n",
    "        return reward if self.agent_color == chess.WHITE else -reward\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\" Renders the current board state (in a human-readable format) \"\"\"\n",
    "        print(self.board)\n",
    "    def _get_material_score(self):\n",
    "        \"\"\"Calculates material balance from White's perspective\"\"\"\n",
    "        piece_values = {\n",
    "            chess.PAWN: 1,\n",
    "            chess.KNIGHT: 3,\n",
    "            chess.BISHOP: 3,\n",
    "            chess.ROOK: 5,\n",
    "            chess.QUEEN: 9\n",
    "        }\n",
    "    \n",
    "        score = 0\n",
    "        for square in chess.SQUARES:\n",
    "            piece = self.board.piece_at(square)\n",
    "            if piece:\n",
    "                value = piece_values.get(piece.piece_type, 0)\n",
    "                score += value if piece.color == chess.WHITE else -value\n",
    "    \n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2708c-2835-4c02-bf7a-e8ced254b0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ChessEnv()\n",
    "n = 0\n",
    "m = []\n",
    "done = False\n",
    "x.reset(chess.WHITE)\n",
    "i=0\n",
    "while not done:\n",
    "    legal_moves = list(x.board.legal_moves)\n",
    "    action = random.randint(0, len(legal_moves) - 1)\n",
    "    obs, reward, done, _ = x.step(action)\n",
    "    if i%2 == 0:\n",
    "        print(\"white's turn\")\n",
    "    else:\n",
    "        print(\"black's turn\")\n",
    "    print(\"Reward:\", reward)\n",
    "    print(\"===\")\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac14a636-4cf0-4c47-a79d-64e844463536",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a89802-0057-42d9-af07-46979d41c3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
